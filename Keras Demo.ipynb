{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll:\n",
    "\n",
    "1. Briefly talk about neural networks, deep learning, and multiclass classification\n",
    "2. Use Keras build models to learn the \"MNIST\" dataset\n",
    "3. We'll first use Logistic Regression to learn a binary dataset\n",
    "3. We'll first use Logistic Regression to learn the full multiclass dataset\n",
    "4. We'll then use a simple neural network very similar to LR called a \"Multilayer Logistic Regression\"\n",
    "5. We'll investigating how changing the number of layers effects the accuracy\n",
    "6. We'll look at a fancier way to combine the layers, and how that can improve performance\n",
    "6. We'll briefly see a fancier network that gets nearly state of the art (99%!) accuracy\n",
    "\n",
    "\n",
    "# Part 1: Background \n",
    "\n",
    "## What are neural networks & deep learning?\n",
    "\n",
    "We'll talk about them in class soon! One good source of explanation is: http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "We've already seen Logistic Regression in class. It's like SVM, but with a different loss.\n",
    "\n",
    "<img src=\"perceptron.png\" width=\"300\">\n",
    "\n",
    "When we used Logistic Regression in class, we classified based on whether the dot product of the example and the weights was *less than 0*. In neural networks this function- which decides whether or not an example is positive based on this dot product- is called an *activation function*. Many neural networks can learn non-linear functions because they use other activation functions such as \"relu\".\n",
    "\n",
    "A neural network is made by chaining a few \"neurons\" togther:\n",
    "\n",
    "<img src=\"nn.png\" width=\"300\">\n",
    "\n",
    "What you need to know:\n",
    "\n",
    "* A neural network is a non-linear classifier, like a decision tree\n",
    "\n",
    "* In practice, they seem to work very well for certain kinds of data including images and text\n",
    "\n",
    "* Neural networks are organized into \"layers\". Lots of choices of how exactly you set up these layer; a \"deep neural network\" is a neural network with many layers.\n",
    "\n",
    "* A perceptron-like classifier is a very simple neural network (a single-layer neural network neural network)\n",
    "\n",
    "\n",
    "## What is Keras & what is it used for?\n",
    "\n",
    "Keras is a library for deep learning, designed to make common types of neural networks very easy to work with.\n",
    "\n",
    "* It's a layer on top of a more powerful, but more difficult to use, library called TensorFlow.\n",
    "\n",
    "* If you want to use \"standard\" neural nets, Keras is better; if you want to use very fancy or complicated models you should use Tensorflow.\n",
    "    \n",
    "* Tons of examples:\n",
    "https://github.com/fchollet/keras/tree/master/examples\n",
    "\n",
    "You can create a model in Keras by specifying which model you want to use, and how you want to train it; Keras takes care of the rest. Keras has many options of \"standard\" models.\n",
    "   \n",
    "   \n",
    "## MNIST\n",
    "\n",
    "Handwritten digit dataset taken from high school students:\n",
    "\n",
    "<img src=\"mnist.png\" width=\"300\">\n",
    "\n",
    "There are 70,000 examples to test and train on, with 10 labels (the digits 0 through 9)\n",
    "\n",
    "The state-of-the art accuracy for this dataset is 99.79% accuracy!\n",
    "\n",
    "### Multi-class classification \n",
    "\n",
    "We'll use \"one v. all\" multiclass because there are 10 labels, we're actually going to need a multi-class classifier.\n",
    "\n",
    "\n",
    "#### Training a 10 way classifier: train 10 binary classifiers\n",
    "What this actually means is we're going to train 10 binary classifier!\n",
    "\n",
    "ie: \n",
    "\n",
    "a \"0 or not 0\" classifier\n",
    "\n",
    "a \"1 or not 1\" classifier\n",
    "\n",
    "a \"2 or not 2\" classifier\n",
    "\n",
    "etc\n",
    "\n",
    "#### Testing with a 10 way classifier: let all the classifiers \"vote\"\n",
    "\n",
    "Then, at test time, each classifier \"votes\" and we choose the label based on the \"most confident\" vote.\n",
    "\n",
    "ie, at test time we have an example that is truely a \"1\"\n",
    "\n",
    "the \"0 or not 0\" classifier returns a negative value (meaning \"it's not a 0!\")\n",
    "\n",
    "the \"1 or not 1\" classifier returns **a positive value** (meaning \"it's a 1!\")\n",
    "\n",
    "the \"2 or not 2\" classifier returns a negative value (meaning \"it's not a 2!\")\n",
    "\n",
    "etc\n",
    "\n",
    "so we label the example as a \"1\".\n",
    "\n",
    "-----\n",
    "\n",
    "#  Setting up the Data\n",
    "\n",
    "Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "\n",
    "Note: use Shift+Enter to run the codeblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib figure size and make inline\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 6]\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our data into a training & testing set\n",
    "\n",
    "`x_train` & `x_test` are the examples, and `y_train` & `y_test` are the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples in full dataset: 60000\n",
      "Number of testing samples in full dataset: 10000\n",
      "\n",
      "Total number of binary 0 / 1 examples:  (14780, 785)\n",
      "Number of training samples in BINARY dataset: 10000\n",
      "Number of testing samples in BINARY dataset: 4780\n"
     ]
    }
   ],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test  = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "# normalize the pixel values to be in the range (0, 1)\n",
    "x_train /= 255 \n",
    "x_test  /= 255\n",
    "\n",
    "\n",
    "print(\"Number of training samples in full dataset: \" + str(x_train.shape[0]))\n",
    "print(\"Number of testing samples in full dataset: \" + str(x_test.shape[0]))\n",
    "\n",
    "# Create a binary version for binary classification\n",
    "df = pd.DataFrame(np.vstack([x_train, x_test]))\n",
    "df['label'] = pd.Series(np.concatenate([y_train, y_test]))\n",
    "binary_data = df[df['label'].isin([0, 1])]\n",
    "\n",
    "\n",
    "binary_x_train = binary_data.iloc[:10000].drop(['label'], axis=1).as_matrix()\n",
    "binary_x_test  = binary_data.iloc[10000:].drop(['label'], axis=1).as_matrix()\n",
    "binary_y_train = binary_data.iloc[:10000]['label'].as_matrix()\n",
    "binary_y_test  = binary_data.iloc[10000:]['label'].as_matrix()\n",
    "\n",
    "print()\n",
    "print(\"Total number of binary 0 / 1 examples: \", binary_data.shape)\n",
    "print(\"Number of training samples in BINARY dataset: \" + str(binary_x_train.shape[0]))\n",
    "print(\"Number of testing samples in BINARY dataset: \" + str(binary_x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out what our data looks like\n",
    "\n",
    "Let's plot one of the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of each image is(784,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e0ea080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[0]\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "image = np.reshape(image, [img_rows, img_cols])\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# The shape of each image is a vector with 784 binary values (\"pixels\")\n",
    "image_shape = x_train[0].shape\n",
    "print(\"The shape of each image is\" + str(image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make our model!!\n",
    "\n",
    "We're going to train a model that Keras calls a \"Sequential\" model.\n",
    "\n",
    "Sequential is an abstraction for really simple networks. There are many ways you can \"link\" the neurons between layers in a neural network, and \"Sequential\" is the simplest way to link these layers. Since we're training a single layer network, this doesn't really concern us.\n",
    "\n",
    "Sidenote: The alternative to in Keras to Sequential models are \"Functional\" models. You can use those to make fancier networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = Sequential()\n",
    "num_classes = 1\n",
    "binary_model.add(Dense(num_classes, activation='sigmoid', input_shape=image_shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Now that we've told the Keras *what the model is*, we now need to tell it *how to learn*.\n",
    "\n",
    "1. What's the loss function?\n",
    "\n",
    "    In class we've seen some loss functions like the hinge loss and the SVM loss. Here we're going to use a different loss function called the \"binary crossentropy\" loss. Cross-entropy is a loss function that works well for learning because it makes learning very fast when your function is \"very wrong\" but slower when it is pretty close to the true function.\n",
    "\n",
    "\n",
    "2. What's the optimizer?\n",
    "\n",
    "    We'll use SGD (Stochastic Gradient Descent) which we've already discussed in class.\n",
    "    \n",
    "\n",
    "3. Which metric to optimize?\n",
    "\n",
    "    We'll use accuracy- which is what we've been using for our algorithms all semester. There are some other options that make sense for other types of datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 4780 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.2880 - acc: 0.9690 - val_loss: 0.1379 - val_acc: 0.9960\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.1092 - acc: 0.9953 - val_loss: 0.0797 - val_acc: 0.9969\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0729 - acc: 0.9960 - val_loss: 0.0576 - val_acc: 0.9973\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 0s 17us/step - loss: 0.0564 - acc: 0.9966 - val_loss: 0.0459 - val_acc: 0.9977\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 0.0469 - acc: 0.9967 - val_loss: 0.0386 - val_acc: 0.9977\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 0s 17us/step - loss: 0.0406 - acc: 0.9968 - val_loss: 0.0336 - val_acc: 0.9979\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 0.0361 - acc: 0.9970 - val_loss: 0.0298 - val_acc: 0.9979\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0328 - acc: 0.9970 - val_loss: 0.0270 - val_acc: 0.9981\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.0301 - acc: 0.9970 - val_loss: 0.0247 - val_acc: 0.9981\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.0280 - acc: 0.9971 - val_loss: 0.0229 - val_acc: 0.9981\n"
     ]
    }
   ],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = binary_model.fit(binary_x_train, binary_y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(binary_x_test, binary_y_test)) #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out that accuracies!\n",
    "\n",
    "99% accuracy!\n",
    "\n",
    "Let's try it on our *test set* instead of our *validation set* now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0228652525319\n",
      "Test accuracy: 0.998117154812\n"
     ]
    }
   ],
   "source": [
    "score = binary_model.evaluate(binary_x_test, binary_y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification!\n",
    "\n",
    "###  Format the labels\n",
    "\n",
    "The labels are currently formatted as numeric values, ie the image above has the label `5`.\n",
    "\n",
    "Since we're training 10 binary classifiers, we need to convert the labels into binary values.\n",
    "\n",
    "ie, the label `1` becomes the array `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`. The entry in the \"1th\" position is true, and all others are false. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of our labels before(60000,)\n",
      "the first 3 labels look like:\n",
      "[5 0 4 1 9]\n",
      "\n",
      "shape of our labels after(60000, 10)\n",
      "the first 3 labels look like:\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# classifying 10 digits\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "print(\"shape of our labels before\" + str(y_train.shape))\n",
    "print(\"the first 3 labels look like:\") \n",
    "print(str(y_train[0:5]) + '\\n')\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test,  num_classes)\n",
    "\n",
    "print(\"shape of our labels after\" + str(y_train.shape))\n",
    "print(\"the first 3 labels look like:\") \n",
    "print(str(y_train[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is good to go, let's train our 10 classifiers!\n",
    "\n",
    "\n",
    "### Let's make our model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_model = Sequential()\n",
    "logistic_reg_model.add(Dense(num_classes, activation='softmax', input_shape=image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model & store accuracies!\n",
    "\n",
    "Note: turning on verbosity can slow things down *a lot* so if you're running some large model, turn it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 1.2563 - acc: 0.7067 - val_loss: 0.8079 - val_acc: 0.8321\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.7133 - acc: 0.8390 - val_loss: 0.6065 - val_acc: 0.8623\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.5863 - acc: 0.8592 - val_loss: 0.5254 - val_acc: 0.8743\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.5250 - acc: 0.8698 - val_loss: 0.4800 - val_acc: 0.8813\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.4876 - acc: 0.8760 - val_loss: 0.4500 - val_acc: 0.8863\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.4617 - acc: 0.8809 - val_loss: 0.4287 - val_acc: 0.8897\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.4426 - acc: 0.8843 - val_loss: 0.4126 - val_acc: 0.8927\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.4278 - acc: 0.8871 - val_loss: 0.3999 - val_acc: 0.8948\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.4157 - acc: 0.8894 - val_loss: 0.3898 - val_acc: 0.8962\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.4057 - acc: 0.8914 - val_loss: 0.3809 - val_acc: 0.8980\n",
      "Test loss: 0.380892626095\n",
      "Test accuracy: 0.898\n"
     ]
    }
   ],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = logistic_reg_model.fit(x_train, y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "score = logistic_reg_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Part 4: learning MNIST with a Multilayer Network!\n",
    "\n",
    "We'll set up a new model\n",
    "\n",
    "TODO ADD DIAGRAM OF WHAT THIS IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(512, activation='relu', input_shape=image_shape))\n",
    "mlp_model.add(Dense(256, activation='relu'))\n",
    "mlp_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the MLP\n",
    "\n",
    "We'll compile with the same parameters as before so that it's a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 1.1063 - acc: 0.7520 - val_loss: 0.5226 - val_acc: 0.8743\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.4504 - acc: 0.8826 - val_loss: 0.3668 - val_acc: 0.9031\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.3581 - acc: 0.9013 - val_loss: 0.3134 - val_acc: 0.9140\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.3168 - acc: 0.9109 - val_loss: 0.2857 - val_acc: 0.9208\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2904 - acc: 0.9177 - val_loss: 0.2656 - val_acc: 0.9268\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.2708 - acc: 0.9241 - val_loss: 0.2493 - val_acc: 0.9310\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2546 - acc: 0.9282 - val_loss: 0.2371 - val_acc: 0.9334\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.2408 - acc: 0.9326 - val_loss: 0.2290 - val_acc: 0.9353\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.2289 - acc: 0.9356 - val_loss: 0.2187 - val_acc: 0.9383\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.2180 - acc: 0.9390 - val_loss: 0.2086 - val_acc: 0.9408\n",
      "Test loss: 0.208557074524\n",
      "Test accuracy: 0.9408\n"
     ]
    }
   ],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = mlp_model.fit(x_train, y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test, y_test))\n",
    "\n",
    "score = mlp_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Part 5: How deep can you go: can adding more layers help us?\n",
    "\n",
    "Creating a deep model gives us some performance gains, but it's not a panacea.  Let's see how model performance behaves as we add more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_accuracy_analysis(x_train, y_train, x_test, y_test, max_layers, layer_size =128, verbose=False):\n",
    "    # type: (np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, int, bool) -> List[float]\n",
    "    \"\"\"This method creates deep feedforward networks of varyig depth from 1 layer to max_layers.  It\n",
    "    returns a list of accuracies on the test set\n",
    "    \n",
    "    Args:\n",
    "        x_train: An np.ndarray of data where each row is a new data point\n",
    "        y_train: An np.ndarray of labels where each row is a binary vector with\n",
    "            1 in the column of the associated class\n",
    "        x_test: An np.ndarray of data where each row is a new data point\n",
    "        y_test: An np.ndarray of labels where each row is a binary vector with\n",
    "            1 in the column of the associated class\n",
    "        max_layers: An int the maximum depth to try\n",
    "        layer_size: The number of neurons in each layer\n",
    "    \n",
    "    Returns:\n",
    "        A List of floats as the accuracies of the model on the test set\n",
    "    \"\"\"\n",
    "    acc_scores = []\n",
    "    \n",
    "    # Build up and test 1 - max layers\n",
    "    for i in range(0, max_layers):\n",
    "        mlp_model = Sequential()\n",
    "        mlp_model.add(Dense(layer_size, activation='relu', input_shape=x_train[0].shape))\n",
    "        \n",
    "        # Add i layers\n",
    "        for j in range(1, i):\n",
    "            mlp_model.add(Dense(layer_size, activation='relu'))\n",
    "\n",
    "        mlp_model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "        mlp_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "        history = mlp_model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=10,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_test, y_test))\n",
    "        score = mlp_model.evaluate(x_test, y_test, verbose=0)\n",
    "        \n",
    "        # Print out scores if verbosity is non 0\n",
    "        if verbose:\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "        acc_scores.append(score)\n",
    "    \n",
    "    return acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6a7d9ac1b324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlayer_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepth_accuracy_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-25e58543cdc2>\u001b[0m in \u001b[0;36mdepth_accuracy_analysis\u001b[0;34m(x_train, y_train, x_test, y_test, max_layers, layer_size, verbose)\u001b[0m\n\u001b[1;32m     34\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                         validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_layers = 30\n",
    "layer_size = 128\n",
    "scores = depth_accuracy_analysis(x_train, y_train, x_test, y_test, max_layers, layer_size=layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, max_layers + 1), map(lambda x: x[1], scores), 'o-')\n",
    "plt.title('Behaviour of model accuracy with depth')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.xlabel('Number of layers with {} nodes/layer'.format(layer_size))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, max_layers + 1), map(lambda x: x[0], scores), 'o-')\n",
    "plt.title('Behaviour of model loss with depth')\n",
    "plt.ylabel('Testing loss')\n",
    "plt.xlabel('Number of layers with {} nodes/layer'.format(layer_size))\n",
    "plt.tight_layout()\n",
    "plt.savefig('depth_experiment_{}-layers_{}-layer_size.png'.format(max_layers, layer_size))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results:\n",
    "\n",
    "<img src=\"depth_experiment_30-layers_128-layer_size.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Part 6: If adding more layers isn't helping, can we get smarter about the network?\n",
    "\n",
    "There are many different types of layers that we can use when building a network.  Right now we have just seen dense layers which are simply perceptron-like nodes stacked together.  Here we will use a set of layers that are well adapted to images called convoluational, and pooling layers.  If you are interested in understanding how these work check out this great paper https://arxiv.org/pdf/1603.07285.pdf.  For now, you can think of the convlutional layers as learning filter like features of increasing complexity.  We will need to redefine our data in order to work with these layers.  Convolutions work on a matrix of values as opposed to a flattened array, because the filters take into consideration local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "image_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=image_shape))\n",
    "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " 8960/60000 [===>..........................] - ETA: 2:38 - loss: 2.0424 - acc: 0.4744"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-afea68e79e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = cnn_model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Part 7: State of the Art-ish Accuracy\n",
    "\n",
    "The state of the art on MNIST is 99.79% accuracy.  With a little extra work we can push our conv net pretty close to this at ~99.16% accuracy.  We can do this by adding regularization in the form of drop out and a better optimizer in the form of ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best CNN\n",
    "best_cnn_model = Sequential()\n",
    "best_cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                   activation='relu',\n",
    "                   input_shape=image_shape))\n",
    "best_cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "best_cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "best_cnn_model.add(Dropout(.5))\n",
    "best_cnn_model.add(Flatten())\n",
    "best_cnn_model.add(Dense(128, activation='relu'))\n",
    "best_cnn_model.add(Dropout(.5))\n",
    "best_cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "best_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_cnn_model.fit(x_train, y_train,\n",
    "                             batch_size=128,\n",
    "                             epochs=10,\n",
    "                             verbose=1,\n",
    "                             validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = best_cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Part 8: Cool stuff with neural nets\n",
    "\n",
    "Michael Nielsen's book is awesome: http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "Lots of keras tutorials: https://github.com/fchollet/keras/tree/master/examples\n",
    "\n",
    "\"Deep art\": https://deepart.io/\n",
    "\n",
    "Twitter bot neural net text generation: https://twitter.com/DeepProverbs\n",
    "\n",
    "Adversarial \n",
    "\n",
    "Generated cat images from outlines: https://affinelayer.com/pixsrv/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
