{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll:\n",
    "\n",
    "1. Briefly talk about neural networks, deep learning, and multiclass classification\n",
    "2. Use Keras build models to learn the \"MNIST\" dataset\n",
    "3. We'll first use Perceptron\n",
    "4. We'll then use a simple neural network very similar to Perceptron called a \"Multilayer Perceptron\"\n",
    "5. We'll investigating how changing the number of layers effects the accuracy\n",
    "6. We'll briefly see a fancier network that gets nearly state of the art (99%!) accuracy\n",
    "\n",
    "\n",
    "# Part 1: Background \n",
    "\n",
    "## What are neural networks & deep learning?\n",
    "\n",
    "We'll talk about them in class soon! One good source of explanation is: http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "We've already seen Perceptron in class\n",
    "\n",
    "<img src=\"perceptron.png\" width=\"300\">\n",
    "\n",
    "A neural network is putting a few Perceptron \"neurons\" chained togther:\n",
    "\n",
    "<img src=\"nn.png\" width=\"300\">\n",
    "\n",
    "What you need to know:\n",
    "\n",
    "* A neural network is a non-linear classifier, like a decision tree\n",
    "\n",
    "* In practice, they seem to work very well for certain kinds of data including images and text\n",
    "\n",
    "* Neural networks are organized into \"layers\". Lots of choices of how exactly you set up these layer; a \"deep neural network\" is a neural network with many layers.\n",
    "\n",
    "* A perceptron is a very simple neural network (a single-layer neural network neural network)\n",
    "\n",
    "\n",
    "## What is Keras & what is it used for?\n",
    "\n",
    "Keras is a library for deep learning, designed to make common types of neural networks very easy to work with.\n",
    "\n",
    "* It's a layer on top of a more powerful, but more difficult to use, library called TensorFlow.\n",
    "\n",
    "* If you want to use \"standard\" neural nets, Keras is better; if you want to use very fancy or complicated models you should use Tensorflow.\n",
    "    \n",
    "* Tons of examples:\n",
    "https://github.com/fchollet/keras/tree/master/examples\n",
    "    \n",
    "-----\n",
    "\n",
    "# Part 2: Learning the MNIST Dataset in Keras: Setting up the Data\n",
    "\n",
    "Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "Handwritten digit dataset taken from high school students:\n",
    "\n",
    "<img src=\"mnist.png\" width=\"300\">\n",
    "\n",
    "There are 70,000 examples to test and train on, with 10 labels (the digits 0 through 9)\n",
    "\n",
    "The state-of-the art accuracy for this dataset is 99.79% accuracy!\n",
    "\n",
    "### Multi-class classification\n",
    "\n",
    "Because there are 10 labels, we're actually going to need a multi-class classifier.\n",
    "\n",
    "\n",
    "#### Training a 10 way classifier: train 10 binary classifiers\n",
    "What this actually means is we're going to train 10 binary perceptrons!\n",
    "\n",
    "ie: \n",
    "\n",
    "a \"0 or not 0\" classifier\n",
    "\n",
    "a \"1 or not 1\" classifier\n",
    "\n",
    "a \"2 or not 2\" classifier\n",
    "\n",
    "etc\n",
    "\n",
    "#### Testing with a 10 way classifier: let all the classifiers \"vote\"\n",
    "\n",
    "Then, at test time, each perceptron \"votes\" and we choose the label based on the \"most confident\" vote.\n",
    "\n",
    "ie, at test time we have an example that is truely a \"1\"\n",
    "\n",
    "the \"0 or not 0\" classifier returns a negative value (meaning \"it's not a 0!\")\n",
    "\n",
    "the \"1 or not 1\" classifier returns **a positive value** (meaning \"it's a 1!\")\n",
    "\n",
    "the \"2 or not 2\" classifier returns a negative value (meaning \"it's not a 2!\")\n",
    "\n",
    "etc\n",
    "\n",
    "so we label the example as a \"1\".\n",
    "\n",
    "\n",
    "### Getting set up with MNIST\n",
    "\n",
    "Note: use Shift+Enter to run the codeblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our data into a training & testing set\n",
    "\n",
    "`x_train` & `x_test` are the examples, and `y_train` & `y_test` are the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 60000\n",
      "Number of testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test  = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "# normalize the pixel values to be in the range (0, 1)\n",
    "x_train /= 255 \n",
    "x_test  /= 255\n",
    "\n",
    "\n",
    "print(\"Number of training samples: \" + str(x_train.shape[0]))\n",
    "print(\"Number of testing samples: \" + str(x_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out what our data looks like\n",
    "\n",
    "Let's plot one of the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of each image is(784,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122a61828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[0]\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "image = np.reshape(image, [img_rows, img_cols])\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# The shape of each image is a vector with 784 binary values (\"pixels\")\n",
    "image_shape = x_train[0].shape\n",
    "print(\"The shape of each image is\" + str(image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great, let's make format our labels\n",
    "\n",
    "The labels are currently formatted as numeric values, ie the image above has the label `5`.\n",
    "\n",
    "Since we're training 10 binary perceptron classifiers, we need to convert the labels into binary values.\n",
    "\n",
    "ie, the label `1` becomes the array `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`. The entry in the \"1th\" position is true, and all others are false. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of our data before(60000,)\n",
      "the first 3 elements look like:\n",
      "[5 0 4 1 9]\n",
      "\n",
      "shape of our data after(60000, 10)\n",
      "the first 3 elements look like:\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# classifying 10 digits\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "print(\"shape of our data before\" + str(y_train.shape))\n",
    "print(\"the first 3 elements look like:\") \n",
    "print(str(y_train[0:5]) + '\\n')\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test,  num_classes)\n",
    "\n",
    "print(\"shape of our data after\" + str(y_train.shape))\n",
    "print(\"the first 3 elements look like:\") \n",
    "print(str(y_train[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Part 3: Using Perceptron to learn MNIST!\n",
    "\n",
    "Now that our data is good to go, let's train our 10 perceptrons!\n",
    "\n",
    "\n",
    "### Let's make our model!!\n",
    "\n",
    "We're going to train a model that Keras calls a \"Sequential\" model.\n",
    "\n",
    "Sequential is an abstraction for really simple networks. There are many ways you can \"link\" the neurons between layers in a neural network, and \"Sequential\" is the simplest way to link these layers. Since we're training perceptrons, which are single layer networks, this doesn't really concern us.\n",
    "\n",
    "Sidenote: The alternative to in Keras to Sequential models are \"Functional\" models. You can use those to make fancier networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "perceptron_model = Sequential()\n",
    "perceptron_model.add(Dense(num_classes, activation='softmax', input_shape=image_shape))\n",
    "\n",
    "perceptron_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: what does this tell us?\n",
    "\n",
    "Since we have a lot more data points (60,000) than parameters to train (7850) we're good to go. --> Maybe this is confusing.\n",
    "\n",
    "todo: maybe visualize\n",
    "\n",
    "### Compiling the model\n",
    "\n",
    "Now that we've told the Keras *what the model is*, we now need to tell it *how to learn*.\n",
    "\n",
    "1. What's the loss function?\n",
    "\n",
    "    In class we've seen some loss functions like the hinge loss and the SVM loss. Here we're going to use a different loss function called the \"categorical crossentropy\" loss. Cross-entropy is a loss function that works well for learning because it makes learning very fast when your function is \"very wrong\" but slower when it is pretty close to the true function.\n",
    "\n",
    "\n",
    "2. What's the optimizer?\n",
    "\n",
    "    We'll use SGD (Stochastic Gradient Descent) which we've already discussed in class.\n",
    "    \n",
    "\n",
    "3. Which metric to optimize?\n",
    "\n",
    "    We'll use accuracy- which is what we've been using for our algorithms all semester. There are some other options that make sense for other types of datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model & store accuracies!\n",
    "\n",
    "Note: turning on verbosity can slow things down *a lot* so if you're running some large model, turn it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3971 - acc: 0.8930 - val_loss: 0.3735 - val_acc: 0.9013\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3898 - acc: 0.8942 - val_loss: 0.3670 - val_acc: 0.9034\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3834 - acc: 0.8955 - val_loss: 0.3615 - val_acc: 0.9039\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.3777 - acc: 0.8967 - val_loss: 0.3565 - val_acc: 0.9049\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3727 - acc: 0.8981 - val_loss: 0.3521 - val_acc: 0.9067\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3682 - acc: 0.8987 - val_loss: 0.3482 - val_acc: 0.9076\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3641 - acc: 0.9001 - val_loss: 0.3442 - val_acc: 0.9086\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.3604 - acc: 0.9006 - val_loss: 0.3411 - val_acc: 0.9094\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.3569 - acc: 0.9013 - val_loss: 0.3380 - val_acc: 0.9101\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3538 - acc: 0.9023 - val_loss: 0.3354 - val_acc: 0.9099\n"
     ]
    }
   ],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = perceptron_model.fit(x_train, y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out those accuracies!\n",
    "\n",
    "90% lookin good!\n",
    "\n",
    "Let's try it on our *test set* instead of our *validation set* now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.335387028074\n",
      "Test accuracy: 0.9099\n"
     ]
    }
   ],
   "source": [
    "score = perceptron_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Part 4: learning MNIST with a Multilayer Perceptron!\n",
    "\n",
    "We'll set up a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(512, activation='relu', input_shape=image_shape))\n",
    "mlp_model.add(Dense(256, activation='relu'))\n",
    "mlp_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the MLP\n",
    "\n",
    "With the same params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 1.0747 - acc: 0.7596 - val_loss: 0.5097 - val_acc: 0.8752\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.4416 - acc: 0.8844 - val_loss: 0.3633 - val_acc: 0.9014\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.3537 - acc: 0.9024 - val_loss: 0.3143 - val_acc: 0.9129\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3139 - acc: 0.9120 - val_loss: 0.2856 - val_acc: 0.9192\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2877 - acc: 0.9190 - val_loss: 0.2660 - val_acc: 0.9250\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2678 - acc: 0.9244 - val_loss: 0.2522 - val_acc: 0.9293\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.2512 - acc: 0.9286 - val_loss: 0.2368 - val_acc: 0.9325\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2373 - acc: 0.9328 - val_loss: 0.2251 - val_acc: 0.9360\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.2249 - acc: 0.9366 - val_loss: 0.2146 - val_acc: 0.9386\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.2137 - acc: 0.9403 - val_loss: 0.2057 - val_acc: 0.9406\n",
      "Test loss: 0.205712091357\n",
      "Test accuracy: 0.9406\n"
     ]
    }
   ],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = mlp_model.fit(x_train, y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test, y_test))\n",
    "\n",
    "score = mlp_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Part 5: Can adding more layers help us?\n",
    "\n",
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Part 6: If adding more layers isn't helping, can we get smarter about the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Part 7: Cool stuff with neural nets\n",
    "\n",
    "Michael Nielsen's book is awesome: http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "Lots of keras tutorials: https://github.com/fchollet/keras/tree/master/examples\n",
    "\n",
    "\"Deep art\": https://deepart.io/\n",
    "\n",
    "Twitter bot neural net text generation: https://twitter.com/DeepProverbs\n",
    "\n",
    "Adversarial \n",
    "\n",
    "Generated cat images from outlines: https://affinelayer.com/pixsrv/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
